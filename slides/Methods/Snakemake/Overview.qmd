## Scaleable workflows with Snakemake

:::: {.columns}
::: {.column}
![](https://snakemake.readthedocs.io/en/stable/_static/logo-snake.svg){fig-align="center" style="margin-bottom:0px"}

- **Workflow management** system for creating reproducible and scalable data analysis pipelines.
- **Python**-based
- Interoperable with hcp cluster or cloud computing **and** local execution.
- Flexible and efficient **job scheduling**
- Supports **containerized workflows**

:::
::: {.column}
::: {.figure}
![](https://f1000research.s3.amazonaws.com/manuscripts/32078/2dd5741c-0e33-41b8-a76b-a26a36ae899c_figure1.gif)
:::
::: {.figure}
![](https://f1000research.s3.amazonaws.com/manuscripts/32078/2dd5741c-0e33-41b8-a76b-a26a36ae899c_figure4.gif)
:::
:::
::::


::: {.notes}
a) Example workflow DAG. The greenish area depicts the jobs that are ready for scheduling (because all input files are present) at a given time during the workflow execution. We assume that the red job at the root generates a temporary file, which may be deleted once all blue jobs are finished.
b) Suboptimal scheduling solution: two green jobs are scheduled, such that only one blue job can be scheduled and the temporary file generated by the red job has to remain on disk until all blue jobs are finished in a subsequent scheduling step.
c) Optimal scheduling solution: the three blue jobs are scheduled, such that the temporary file generated by the red job can be deleted afterwards.
:::